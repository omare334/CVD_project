{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class for importing ECG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataclass for importing in ecg data\n",
    "## channels : imports in the label for each channels in the file\n",
    "### sample-freq gives the frequency here it is 1000hz\n",
    "#### data actually gives the ecg data at each millisecond\n",
    "class TxtFile:\n",
    "    def __init__(self, filepath, verbose=False):\n",
    "        self.filepath = filepath\n",
    "        self.channels, self.sample_freq, self.data = self.load_file()\n",
    "        if verbose: print(\"Channels: {}\".format(self.channels))\n",
    "\n",
    "    def load_file(self):\n",
    "        with open(self.filepath) as f:\n",
    "            channels, sample_freq = self.load_channels(f)\n",
    "            _ = self._read_until(f, \"[Data]\")\n",
    "            data = f.read()\n",
    "            data = pd.read_table(StringIO(data), names=channels, sep=',')\n",
    "            # data = self.filter_data(data)\n",
    "            return channels, sample_freq, data\n",
    "\n",
    "    def load_channels(self, file):\n",
    "        channels = []\n",
    "        line = self._read_until(file, \"Channels exported:\")\n",
    "        sample_freq = int(self._read_until(file, \"Sample Rate\").rsplit(' ', 1)[-1].rsplit('Hz')[0])\n",
    "        n_channels = int(line.split(' ')[-1])\n",
    "        for n_channel in range(n_channels):\n",
    "            line = self._read_until(file, \"Label:\")\n",
    "            channel_name = line.replace('Label: ', '').rstrip()\n",
    "            channels.append(channel_name)\n",
    "        return channels, sample_freq\n",
    "\n",
    "    @staticmethod\n",
    "    def _read_until(file, string):\n",
    "        line = file.readline()\n",
    "        while string not in line:\n",
    "            line = file.readline()\n",
    "        return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels: ['I', 'aVF', 'V1', 'V6', 'CS 1-2', 'CS 3-4', 'CS 5-6', 'CS 7-8', 'CS 9-10']\n"
     ]
    }
   ],
   "source": [
    "#loading in the file to see if the file path is correct \n",
    "file_path = \"/rds/general/project/fsn-ai-ecg-data/live/afml/CSCL_1st/CS/CSCL1_post_cs_1.txt\"\n",
    "txt_file = TxtFile(file_path, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I</th>\n",
       "      <th>aVF</th>\n",
       "      <th>V1</th>\n",
       "      <th>V6</th>\n",
       "      <th>CS 1-2</th>\n",
       "      <th>CS 3-4</th>\n",
       "      <th>CS 5-6</th>\n",
       "      <th>CS 7-8</th>\n",
       "      <th>CS 9-10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>-176</td>\n",
       "      <td>-480</td>\n",
       "      <td>-320</td>\n",
       "      <td>-32</td>\n",
       "      <td>-144</td>\n",
       "      <td>144</td>\n",
       "      <td>80</td>\n",
       "      <td>-96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>-160</td>\n",
       "      <td>-480</td>\n",
       "      <td>-272</td>\n",
       "      <td>-16</td>\n",
       "      <td>-48</td>\n",
       "      <td>224</td>\n",
       "      <td>112</td>\n",
       "      <td>-128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48</td>\n",
       "      <td>-128</td>\n",
       "      <td>-480</td>\n",
       "      <td>-256</td>\n",
       "      <td>48</td>\n",
       "      <td>-48</td>\n",
       "      <td>128</td>\n",
       "      <td>80</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "      <td>-96</td>\n",
       "      <td>-496</td>\n",
       "      <td>-224</td>\n",
       "      <td>-48</td>\n",
       "      <td>-112</td>\n",
       "      <td>176</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>-48</td>\n",
       "      <td>-496</td>\n",
       "      <td>-208</td>\n",
       "      <td>-32</td>\n",
       "      <td>-128</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>-64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>-464</td>\n",
       "      <td>-1216</td>\n",
       "      <td>-16</td>\n",
       "      <td>-1184</td>\n",
       "      <td>144</td>\n",
       "      <td>48</td>\n",
       "      <td>-48</td>\n",
       "      <td>0</td>\n",
       "      <td>-48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>-480</td>\n",
       "      <td>-1200</td>\n",
       "      <td>-16</td>\n",
       "      <td>-1184</td>\n",
       "      <td>112</td>\n",
       "      <td>-96</td>\n",
       "      <td>32</td>\n",
       "      <td>-128</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>-496</td>\n",
       "      <td>-1200</td>\n",
       "      <td>32</td>\n",
       "      <td>-1168</td>\n",
       "      <td>80</td>\n",
       "      <td>16</td>\n",
       "      <td>-48</td>\n",
       "      <td>-80</td>\n",
       "      <td>-176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>-496</td>\n",
       "      <td>-1168</td>\n",
       "      <td>16</td>\n",
       "      <td>-1168</td>\n",
       "      <td>-112</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>-80</td>\n",
       "      <td>-144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>-496</td>\n",
       "      <td>-1168</td>\n",
       "      <td>64</td>\n",
       "      <td>-1136</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>-64</td>\n",
       "      <td>-96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         I   aVF   V1    V6  CS 1-2  CS 3-4  CS 5-6  CS 7-8  CS 9-10\n",
       "0       64  -176 -480  -320     -32    -144     144      80      -96\n",
       "1       64  -160 -480  -272     -16     -48     224     112     -128\n",
       "2       48  -128 -480  -256      48     -48     128      80       64\n",
       "3       48   -96 -496  -224     -48    -112     176      32       64\n",
       "4       32   -48 -496  -208     -32    -128     112       0      -64\n",
       "...    ...   ...  ...   ...     ...     ...     ...     ...      ...\n",
       "59995 -464 -1216  -16 -1184     144      48     -48       0      -48\n",
       "59996 -480 -1200  -16 -1184     112     -96      32    -128        0\n",
       "59997 -496 -1200   32 -1168      80      16     -48     -80     -176\n",
       "59998 -496 -1168   16 -1168    -112       0      80     -80     -144\n",
       "59999 -496 -1168   64 -1136      96      96       0     -64      -96\n",
       "\n",
       "[60000 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_file.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -128,  -144,  -128, ...,    32,    96,   128],\n",
       "       [  352,   208,   -32, ...,  -112,   -48,   -16],\n",
       "       [   32,    48,   128, ...,   -32,   -32,     0],\n",
       "       ...,\n",
       "       [  -32,   -64,   -16, ..., -4272, -4496, -4144],\n",
       "       [-3072, -1616, -1024, ...,  -144,  -112,   -96],\n",
       "       [  -64,   -64,   -16, ...,  -224,  -176,  -128]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(\"/rds/general/user/oe222/home/AF_ecg/Preprocessing/CS_training.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_file.sample_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating a df with file path , procdure and patient numbers in the cscl_1st directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             File Path Patient Number  \\\n",
      "0    /rds/general/project/fsn-ai-ecg-data/live/afml...             48   \n",
      "1    /rds/general/project/fsn-ai-ecg-data/live/afml...             60   \n",
      "2    /rds/general/project/fsn-ai-ecg-data/live/afml...             15   \n",
      "3    /rds/general/project/fsn-ai-ecg-data/live/afml...             81   \n",
      "4    /rds/general/project/fsn-ai-ecg-data/live/afml...             49   \n",
      "..                                                 ...            ...   \n",
      "591  /rds/general/project/fsn-ai-ecg-data/live/afml...             29   \n",
      "592  /rds/general/project/fsn-ai-ecg-data/live/afml...             41   \n",
      "593  /rds/general/project/fsn-ai-ecg-data/live/afml...             71   \n",
      "594  /rds/general/project/fsn-ai-ecg-data/live/afml...             72   \n",
      "595  /rds/general/project/fsn-ai-ecg-data/live/afml...             73   \n",
      "\n",
      "    Procedure  \n",
      "0        post  \n",
      "1         pre  \n",
      "2         pre  \n",
      "3         pre  \n",
      "4        post  \n",
      "..        ...  \n",
      "591       pre  \n",
      "592       pre  \n",
      "593       pre  \n",
      "594       pre  \n",
      "595       pre  \n",
      "\n",
      "[596 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "pd.reset_option(\"display.max_rows\")\n",
    "directory = \"/rds/general/project/fsn-ai-ecg-data/live/afml/CSCL_1st/CS/\"\n",
    "\n",
    "file_paths = []  # List to store file paths\n",
    "patient_numbers = []  # List to store patient numbers\n",
    "procedures = []  # List to store procedures\n",
    "\n",
    "# Regular expression pattern to extract the patient number and procedure\n",
    "pattern = r\"CSCL(\\d+)_(pre|post)_\"\n",
    "\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        file_paths.append(file_path)\n",
    "\n",
    "        # Extract patient number and procedure from the file name using regex\n",
    "        match = re.search(pattern, file)\n",
    "        if match:\n",
    "            patient_number = match.group(1)\n",
    "            procedure = match.group(2)\n",
    "            patient_numbers.append(patient_number)\n",
    "            procedures.append(procedure)\n",
    "        else:\n",
    "            patient_numbers.append(None)\n",
    "            procedures.append(None)\n",
    "\n",
    "# Create a DataFrame from the file_paths, patient_numbers, and procedures lists\n",
    "df = pd.DataFrame({\"File Path\": file_paths, \"Patient Number\": patient_numbers, \"Procedure\": procedures})\n",
    "\n",
    "print(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the patients in the cscl_redo directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             File Path Patient Number  \\\n",
      "0    /rds/general/project/fsn-ai-ecg-data/live/afml...             48   \n",
      "1    /rds/general/project/fsn-ai-ecg-data/live/afml...             60   \n",
      "2    /rds/general/project/fsn-ai-ecg-data/live/afml...             15   \n",
      "3    /rds/general/project/fsn-ai-ecg-data/live/afml...             81   \n",
      "4    /rds/general/project/fsn-ai-ecg-data/live/afml...             49   \n",
      "..                                                 ...            ...   \n",
      "711  /rds/general/project/fsn-ai-ecg-data/live/afml...            105   \n",
      "712  /rds/general/project/fsn-ai-ecg-data/live/afml...            104   \n",
      "713  /rds/general/project/fsn-ai-ecg-data/live/afml...            103   \n",
      "714  /rds/general/project/fsn-ai-ecg-data/live/afml...             96   \n",
      "715  /rds/general/project/fsn-ai-ecg-data/live/afml...             98   \n",
      "\n",
      "    Procedure  \n",
      "0        post  \n",
      "1         pre  \n",
      "2         pre  \n",
      "3         pre  \n",
      "4        post  \n",
      "..        ...  \n",
      "711       pre  \n",
      "712       pre  \n",
      "713       pre  \n",
      "714       pre  \n",
      "715       pre  \n",
      "\n",
      "[716 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "directory = \"/rds/general/project/fsn-ai-ecg-data/live/afml/CSCL_redo/\"\n",
    "# Regular expression pattern to extract the patient number and procedure\n",
    "pattern = r\"CSCL(\\d+)_(pre|post)_\"\n",
    "\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        file_paths.append(file_path)\n",
    "\n",
    "        # Extract patient number and procedure from the file name using regex\n",
    "        match = re.search(pattern, file)\n",
    "        if match:\n",
    "            patient_number = match.group(1)\n",
    "            procedure = match.group(2)\n",
    "            patient_numbers.append(patient_number)\n",
    "            procedures.append(procedure)\n",
    "        else:\n",
    "            patient_numbers.append(None)\n",
    "            procedures.append(None)\n",
    "\n",
    "# Create a DataFrame from the file_paths, patient_numbers, and procedures lists\n",
    "df = pd.DataFrame({\"File Path\": file_paths, \"Patient Number\": patient_numbers, \"Procedure\": procedures})\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding patients from the RRAF direcotry \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              File Path Patient Number  \\\n",
      "0     /rds/general/project/fsn-ai-ecg-data/live/afml...             48   \n",
      "1     /rds/general/project/fsn-ai-ecg-data/live/afml...             60   \n",
      "2     /rds/general/project/fsn-ai-ecg-data/live/afml...             15   \n",
      "3     /rds/general/project/fsn-ai-ecg-data/live/afml...             81   \n",
      "4     /rds/general/project/fsn-ai-ecg-data/live/afml...             49   \n",
      "...                                                 ...            ...   \n",
      "2411  /rds/general/project/fsn-ai-ecg-data/live/afml...          196.0   \n",
      "2412  /rds/general/project/fsn-ai-ecg-data/live/afml...          181.0   \n",
      "2413  /rds/general/project/fsn-ai-ecg-data/live/afml...          133.0   \n",
      "2414  /rds/general/project/fsn-ai-ecg-data/live/afml...          208.0   \n",
      "2415  /rds/general/project/fsn-ai-ecg-data/live/afml...          244.0   \n",
      "\n",
      "     Procedure  \n",
      "0         post  \n",
      "1          pre  \n",
      "2          pre  \n",
      "3          pre  \n",
      "4         post  \n",
      "...        ...  \n",
      "2411      post  \n",
      "2412       pre  \n",
      "2413       pre  \n",
      "2414      post  \n",
      "2415       pre  \n",
      "\n",
      "[2416 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "directory = \"/rds/general/project/fsn-ai-ecg-data/live/afml/RRAF/CS/\"\n",
    "pattern = r\"RRAF(\\d+)_(pre|post)_\"\n",
    "\n",
    "file_paths = []  # List to store file paths\n",
    "patient_numbers = []  # List to store patient numbers\n",
    "procedures = []  # List to store procedures\n",
    "\n",
    "existing_patient_numbers = set(df[\"Patient Number\"]) if \"Patient Number\" in df.columns else set()\n",
    "\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        file_paths.append(file_path)\n",
    "\n",
    "        # Extract patient number and procedure from the file name using regex\n",
    "        match = re.search(pattern, file)\n",
    "        if match:\n",
    "            patient_number = int(match.group(1)) + 107\n",
    "            while patient_number in existing_patient_numbers:\n",
    "                patient_number += 107\n",
    "            procedure = match.group(2)\n",
    "            patient_numbers.append(int(patient_number))  # Convert to integer\n",
    "            procedures.append(procedure)\n",
    "        else:\n",
    "            patient_numbers.append(None)\n",
    "            procedures.append(None)\n",
    "\n",
    "# Create a DataFrame from the file_paths, patient_numbers, and procedures lists\n",
    "new_df = pd.DataFrame({\"File Path\": file_paths, \"Patient Number\": patient_numbers, \"Procedure\": procedures})\n",
    "\n",
    "# Concatenate the new_df with the previous df\n",
    "df = pd.concat([df, new_df], ignore_index=True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with NaN values:\n",
      "                                              File Path Patient Number  \\\n",
      "1709  /rds/general/project/fsn-ai-ecg-data/live/afml...            NaN   \n",
      "\n",
      "     Procedure  \n",
      "1709      None  \n"
     ]
    }
   ],
   "source": [
    "#finding a removing rows with nas \n",
    "rows_with_nas = df[df.isna().any(axis=1)]\n",
    "print(\"Rows with NaN values:\")\n",
    "print(rows_with_nas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping na row \n",
    "df = df.drop(1709)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving csv\n",
    "df.to_csv(\"output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique patients: 285\n",
      "\n",
      "number of ecg samples per patient:\n",
      "197.0    20\n",
      "48       10\n",
      "201.0    10\n",
      "163.0    10\n",
      "208.0    10\n",
      "260.0    10\n",
      "146.0    10\n",
      "302.0    10\n",
      "116.0    10\n",
      "226.0    10\n",
      "253.0    10\n",
      "249.0    10\n",
      "184.0    10\n",
      "296.0    10\n",
      "110.0    10\n",
      "140.0    10\n",
      "125.0    10\n",
      "155.0    10\n",
      "256.0    10\n",
      "278.0    10\n",
      "143.0    10\n",
      "281.0    10\n",
      "251.0    10\n",
      "219.0    10\n",
      "242.0    10\n",
      "115.0    10\n",
      "228.0    10\n",
      "233.0    10\n",
      "179.0    10\n",
      "203.0    10\n",
      "280.0    10\n",
      "190.0    10\n",
      "178.0    10\n",
      "199.0    10\n",
      "211.0    10\n",
      "160.0    10\n",
      "174.0    10\n",
      "172.0    10\n",
      "113.0    10\n",
      "159.0    10\n",
      "284.0    10\n",
      "263.0    10\n",
      "193.0    10\n",
      "266.0    10\n",
      "237.0    10\n",
      "252.0    10\n",
      "176.0    10\n",
      "216.0    10\n",
      "196.0    10\n",
      "170.0    10\n",
      "167.0    10\n",
      "202.0    10\n",
      "308.0    10\n",
      "132.0    10\n",
      "192.0    10\n",
      "177.0    10\n",
      "303.0    10\n",
      "200.0    10\n",
      "152.0    10\n",
      "217.0    10\n",
      "229.0    10\n",
      "188.0    10\n",
      "123.0    10\n",
      "129.0    10\n",
      "153.0    10\n",
      "269.0    10\n",
      "295.0    10\n",
      "111.0    10\n",
      "145.0    10\n",
      "257.0    10\n",
      "137.0    10\n",
      "204.0    10\n",
      "258.0    10\n",
      "235.0    10\n",
      "275.0    10\n",
      "240.0    10\n",
      "248.0    10\n",
      "114.0    10\n",
      "142.0    10\n",
      "279.0    10\n",
      "130.0    10\n",
      "245.0    10\n",
      "134.0    10\n",
      "139.0    10\n",
      "112.0    10\n",
      "220.0    10\n",
      "254.0    10\n",
      "244.0    10\n",
      "126.0    10\n",
      "241.0    10\n",
      "151.0    10\n",
      "262.0    10\n",
      "205.0    10\n",
      "243.0    10\n",
      "238.0    10\n",
      "265.0    10\n",
      "157.0    10\n",
      "232.0    10\n",
      "218.0    10\n",
      "117.0    10\n",
      "108.0    10\n",
      "144.0    10\n",
      "158.0    10\n",
      "162.0    10\n",
      "247.0    10\n",
      "224.0    10\n",
      "210.0    10\n",
      "165.0    10\n",
      "124.0    10\n",
      "282.0    10\n",
      "150.0    10\n",
      "127.0    10\n",
      "185.0    10\n",
      "168.0    10\n",
      "209.0    10\n",
      "187.0    10\n",
      "214.0    10\n",
      "236.0    10\n",
      "181.0    10\n",
      "222.0    10\n",
      "175.0    10\n",
      "264.0    10\n",
      "135.0    10\n",
      "301.0    10\n",
      "118.0    10\n",
      "141.0    10\n",
      "286.0    10\n",
      "24       10\n",
      "46       10\n",
      "25       10\n",
      "74       10\n",
      "11       10\n",
      "44       10\n",
      "47       10\n",
      "23       10\n",
      "42       10\n",
      "5        10\n",
      "41       10\n",
      "8        10\n",
      "49       10\n",
      "81       10\n",
      "102      10\n",
      "99       10\n",
      "17       10\n",
      "43       10\n",
      "101      10\n",
      "64       10\n",
      "1        10\n",
      "75       10\n",
      "32       10\n",
      "80       10\n",
      "70       10\n",
      "67       10\n",
      "6        10\n",
      "51       10\n",
      "71       10\n",
      "7        10\n",
      "33       10\n",
      "69       10\n",
      "20       10\n",
      "2        10\n",
      "50       10\n",
      "98       10\n",
      "36       10\n",
      "104      10\n",
      "183.0    10\n",
      "288.0    10\n",
      "239.0    10\n",
      "121.0    10\n",
      "100      10\n",
      "147.0    10\n",
      "138.0    10\n",
      "191.0    10\n",
      "133.0    10\n",
      "186.0    10\n",
      "207.0    10\n",
      "194.0    10\n",
      "120.0    10\n",
      "283.0    10\n",
      "304.0    10\n",
      "250.0    10\n",
      "156.0    10\n",
      "223.0    10\n",
      "148.0    10\n",
      "285.0    10\n",
      "206.0    10\n",
      "166.0    10\n",
      "122.0    10\n",
      "68       10\n",
      "294.0    10\n",
      "131.0    10\n",
      "96       10\n",
      "103      10\n",
      "95       10\n",
      "97       10\n",
      "106      10\n",
      "119.0    10\n",
      "105      10\n",
      "246.0     9\n",
      "76        9\n",
      "180.0     9\n",
      "225.0     8\n",
      "270.0     6\n",
      "15        5\n",
      "56        5\n",
      "290.0     5\n",
      "274.0     5\n",
      "297.0     5\n",
      "39        5\n",
      "305.0     5\n",
      "276.0     5\n",
      "271.0     5\n",
      "289.0     5\n",
      "299.0     5\n",
      "300.0     5\n",
      "59        5\n",
      "30        5\n",
      "149.0     5\n",
      "287.0     5\n",
      "272.0     5\n",
      "16        5\n",
      "10        5\n",
      "198.0     5\n",
      "277.0     5\n",
      "231.0     5\n",
      "78        5\n",
      "182.0     5\n",
      "213.0     5\n",
      "259.0     5\n",
      "57        5\n",
      "215.0     5\n",
      "261.0     5\n",
      "4         5\n",
      "35        5\n",
      "14        5\n",
      "136.0     5\n",
      "52        5\n",
      "88        5\n",
      "66        5\n",
      "9         5\n",
      "29        5\n",
      "72        5\n",
      "40        5\n",
      "86        5\n",
      "291.0     5\n",
      "18        5\n",
      "79        5\n",
      "77        5\n",
      "63        5\n",
      "38        5\n",
      "267.0     5\n",
      "62        5\n",
      "171.0     5\n",
      "21        5\n",
      "298.0     5\n",
      "227.0     5\n",
      "53        5\n",
      "58        5\n",
      "73        5\n",
      "28        5\n",
      "19        5\n",
      "273.0     5\n",
      "164.0     5\n",
      "84        5\n",
      "45        5\n",
      "3         5\n",
      "37        5\n",
      "85        5\n",
      "27        5\n",
      "234.0     5\n",
      "65        5\n",
      "34        5\n",
      "26        4\n",
      "82        4\n",
      "13        4\n",
      "12        4\n",
      "55        4\n",
      "60        3\n",
      "31        3\n",
      "22        3\n",
      "61        3\n",
      "54        2\n",
      "83        2\n",
      "255.0     2\n",
      "87        1\n",
      "Name: Patient Number, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#looking at the counts of patients and how many ecg samples have been taken \n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "# Count unique patients and their frequencies\n",
    "unique_patients = df[\"Patient Number\"].nunique()\n",
    "patient_frequencies = df[\"Patient Number\"].value_counts()\n",
    "\n",
    "print(\"Number of unique patients:\", unique_patients)\n",
    "print(\"\\nnumber of ecg samples per patient:\")\n",
    "print(patient_frequencies)\n",
    "pd.reset_option(\"display.max_rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some pateints do not even have more than one ECG reeading now I wil check hwo many people do not have pre and post readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Patient Number Procedure  Count\n",
      "0            108.0      post      5\n",
      "1            108.0       pre      5\n",
      "2            110.0      post      5\n",
      "3            110.0       pre      5\n",
      "4            111.0      post      5\n",
      "5            111.0       pre      5\n",
      "6            112.0      post      5\n",
      "7            112.0       pre      5\n",
      "8            113.0      post      5\n",
      "9            113.0       pre      5\n",
      "10           114.0      post      5\n",
      "11           114.0       pre      5\n",
      "12           115.0      post      5\n",
      "13           115.0       pre      5\n",
      "14           116.0      post      5\n",
      "15           116.0       pre      5\n",
      "16           117.0      post      5\n",
      "17           117.0       pre      5\n",
      "18           118.0      post      5\n",
      "19           118.0       pre      5\n",
      "20           119.0      post      5\n",
      "21           119.0       pre      5\n",
      "22           120.0      post      5\n",
      "23           120.0       pre      5\n",
      "24           121.0      post      5\n",
      "25           121.0       pre      5\n",
      "26           122.0      post      5\n",
      "27           122.0       pre      5\n",
      "28           123.0      post      5\n",
      "29           123.0       pre      5\n",
      "30           124.0      post      5\n",
      "31           124.0       pre      5\n",
      "32           125.0      post      5\n",
      "33           125.0       pre      5\n",
      "34           126.0      post      5\n",
      "35           126.0       pre      5\n",
      "36           127.0      post      5\n",
      "37           127.0       pre      5\n",
      "38           129.0      post      5\n",
      "39           129.0       pre      5\n",
      "40           130.0      post      5\n",
      "41           130.0       pre      5\n",
      "42           131.0      post      5\n",
      "43           131.0       pre      5\n",
      "44           132.0      post      5\n",
      "45           132.0       pre      5\n",
      "46           133.0      post      5\n",
      "47           133.0       pre      5\n",
      "48           134.0      post      5\n",
      "49           134.0       pre      5\n",
      "50           135.0      post      5\n",
      "51           135.0       pre      5\n",
      "52           136.0       pre      5\n",
      "53           137.0      post      5\n",
      "54           137.0       pre      5\n",
      "55           138.0      post      5\n",
      "56           138.0       pre      5\n",
      "57           139.0      post      5\n",
      "58           139.0       pre      5\n",
      "59           140.0      post      5\n",
      "60           140.0       pre      5\n",
      "61           141.0      post      5\n",
      "62           141.0       pre      5\n",
      "63           142.0      post      5\n",
      "64           142.0       pre      5\n",
      "65           143.0      post      5\n",
      "66           143.0       pre      5\n",
      "67           144.0      post      5\n",
      "68           144.0       pre      5\n",
      "69           145.0      post      5\n",
      "70           145.0       pre      5\n",
      "71           146.0      post      5\n",
      "72           146.0       pre      5\n",
      "73           147.0      post      5\n",
      "74           147.0       pre      5\n",
      "75           148.0      post      5\n",
      "76           148.0       pre      5\n",
      "77           149.0       pre      5\n",
      "78           150.0      post      5\n",
      "79           150.0       pre      5\n",
      "80           151.0      post      5\n",
      "81           151.0       pre      5\n",
      "82           152.0      post      5\n",
      "83           152.0       pre      5\n",
      "84           153.0      post      5\n",
      "85           153.0       pre      5\n",
      "86           155.0      post      5\n",
      "87           155.0       pre      5\n",
      "88           156.0      post      5\n",
      "89           156.0       pre      5\n",
      "90           157.0      post      5\n",
      "91           157.0       pre      5\n",
      "92           158.0      post      5\n",
      "93           158.0       pre      5\n",
      "94           159.0      post      5\n",
      "95           159.0       pre      5\n",
      "96           160.0      post      5\n",
      "97           160.0       pre      5\n",
      "98           162.0      post      5\n",
      "99           162.0       pre      5\n",
      "100          163.0      post      5\n",
      "101          163.0       pre      5\n",
      "102          164.0       pre      5\n",
      "103          165.0      post      5\n",
      "104          165.0       pre      5\n",
      "105          166.0      post      5\n",
      "106          166.0       pre      5\n",
      "107          167.0      post      5\n",
      "108          167.0       pre      5\n",
      "109          168.0      post      5\n",
      "110          168.0       pre      5\n",
      "111          170.0      post      5\n",
      "112          170.0       pre      5\n",
      "113          171.0       pre      5\n",
      "114          172.0      post      5\n",
      "115          172.0       pre      5\n",
      "116          174.0      post      5\n",
      "117          174.0       pre      5\n",
      "118          175.0      post      5\n",
      "119          175.0       pre      5\n",
      "120          176.0      post      5\n",
      "121          176.0       pre      5\n",
      "122          177.0      post      5\n",
      "123          177.0       pre      5\n",
      "124          178.0      post      5\n",
      "125          178.0       pre      5\n",
      "126          179.0      post      5\n",
      "127          179.0       pre      5\n",
      "128          180.0      post      4\n",
      "129          180.0       pre      5\n",
      "130          181.0      post      5\n",
      "131          181.0       pre      5\n",
      "132          182.0       pre      5\n",
      "133          183.0      post      5\n",
      "134          183.0       pre      5\n",
      "135          184.0      post      5\n",
      "136          184.0       pre      5\n",
      "137          185.0      post      5\n",
      "138          185.0       pre      5\n",
      "139          186.0      post      5\n",
      "140          186.0       pre      5\n",
      "141          187.0      post      5\n",
      "142          187.0       pre      5\n",
      "143          188.0      post      5\n",
      "144          188.0       pre      5\n",
      "145          190.0      post      5\n",
      "146          190.0       pre      5\n",
      "147          191.0      post      5\n",
      "148          191.0       pre      5\n",
      "149          192.0      post      5\n",
      "150          192.0       pre      5\n",
      "151          193.0      post      5\n",
      "152          193.0       pre      5\n",
      "153          194.0      post      5\n",
      "154          194.0       pre      5\n",
      "155          196.0      post      5\n",
      "156          196.0       pre      5\n",
      "157          197.0      post     10\n",
      "158          197.0       pre     10\n",
      "159          198.0       pre      5\n",
      "160          199.0      post      5\n",
      "161          199.0       pre      5\n",
      "162          200.0      post      5\n",
      "163          200.0       pre      5\n",
      "164          201.0      post      5\n",
      "165          201.0       pre      5\n",
      "166          202.0      post      5\n",
      "167          202.0       pre      5\n",
      "168          203.0      post      5\n",
      "169          203.0       pre      5\n",
      "170          204.0      post      5\n",
      "171          204.0       pre      5\n",
      "172          205.0      post      5\n",
      "173          205.0       pre      5\n",
      "174          206.0      post      5\n",
      "175          206.0       pre      5\n",
      "176          207.0      post      5\n",
      "177          207.0       pre      5\n",
      "178          208.0      post      5\n",
      "179          208.0       pre      5\n",
      "180          209.0      post      5\n",
      "181          209.0       pre      5\n",
      "182          210.0      post      5\n",
      "183          210.0       pre      5\n",
      "184          211.0      post      5\n",
      "185          211.0       pre      5\n",
      "186          213.0       pre      5\n",
      "187          214.0      post      5\n",
      "188          214.0       pre      5\n",
      "189          215.0       pre      5\n",
      "190          216.0      post      5\n",
      "191          216.0       pre      5\n",
      "192          217.0      post      5\n",
      "193          217.0       pre      5\n",
      "194          218.0      post      5\n",
      "195          218.0       pre      5\n",
      "196          219.0      post      5\n",
      "197          219.0       pre      5\n",
      "198          220.0      post      5\n",
      "199          220.0       pre      5\n",
      "200          222.0      post      5\n",
      "201          222.0       pre      5\n",
      "202          223.0      post      5\n",
      "203          223.0       pre      5\n",
      "204          224.0      post      5\n",
      "205          224.0       pre      5\n",
      "206          225.0      post      3\n",
      "207          225.0       pre      5\n",
      "208          226.0      post      5\n",
      "209          226.0       pre      5\n",
      "210          227.0       pre      5\n",
      "211          228.0      post      5\n",
      "212          228.0       pre      5\n",
      "213          229.0      post      5\n",
      "214          229.0       pre      5\n",
      "215          231.0       pre      5\n",
      "216          232.0      post      5\n",
      "217          232.0       pre      5\n",
      "218          233.0      post      5\n",
      "219          233.0       pre      5\n",
      "220          234.0       pre      5\n",
      "221          235.0      post      5\n",
      "222          235.0       pre      5\n",
      "223          236.0      post      5\n",
      "224          236.0       pre      5\n",
      "225          237.0      post      5\n",
      "226          237.0       pre      5\n",
      "227          238.0      post      5\n",
      "228          238.0       pre      5\n",
      "229          239.0      post      5\n",
      "230          239.0       pre      5\n",
      "231          240.0      post      5\n",
      "232          240.0       pre      5\n",
      "233          241.0      post      5\n",
      "234          241.0       pre      5\n",
      "235          242.0      post      5\n",
      "236          242.0       pre      5\n",
      "237          243.0      post      5\n",
      "238          243.0       pre      5\n",
      "239          244.0      post      5\n",
      "240          244.0       pre      5\n",
      "241          245.0      post      5\n",
      "242          245.0       pre      5\n",
      "243          246.0      post      5\n",
      "244          246.0       pre      4\n",
      "245          247.0      post      5\n",
      "246          247.0       pre      5\n",
      "247          248.0      post      5\n",
      "248          248.0       pre      5\n",
      "249          249.0      post      5\n",
      "250          249.0       pre      5\n",
      "251          250.0      post      5\n",
      "252          250.0       pre      5\n",
      "253          251.0      post      5\n",
      "254          251.0       pre      5\n",
      "255          252.0      post      5\n",
      "256          252.0       pre      5\n",
      "257          253.0      post      5\n",
      "258          253.0       pre      5\n",
      "259          254.0      post      5\n",
      "260          254.0       pre      5\n",
      "261          255.0       pre      2\n",
      "262          256.0      post      5\n",
      "263          256.0       pre      5\n",
      "264          257.0      post      5\n",
      "265          257.0       pre      5\n",
      "266          258.0      post      5\n",
      "267          258.0       pre      5\n",
      "268          259.0       pre      5\n",
      "269          260.0      post      5\n",
      "270          260.0       pre      5\n",
      "271          261.0       pre      5\n",
      "272          262.0      post      5\n",
      "273          262.0       pre      5\n",
      "274          263.0      post      5\n",
      "275          263.0       pre      5\n",
      "276          264.0      post      5\n",
      "277          264.0       pre      5\n",
      "278          265.0      post      5\n",
      "279          265.0       pre      5\n",
      "280          266.0      post      5\n",
      "281          266.0       pre      5\n",
      "282          267.0       pre      5\n",
      "283          269.0      post      5\n",
      "284          269.0       pre      5\n",
      "285          270.0      post      1\n",
      "286          270.0       pre      5\n",
      "287          271.0       pre      5\n",
      "288          272.0       pre      5\n",
      "289          273.0       pre      5\n",
      "290          274.0       pre      5\n",
      "291          275.0      post      5\n",
      "292          275.0       pre      5\n",
      "293          276.0       pre      5\n",
      "294          277.0       pre      5\n",
      "295          278.0      post      5\n",
      "296          278.0       pre      5\n",
      "297          279.0      post      5\n",
      "298          279.0       pre      5\n",
      "299          280.0      post      5\n",
      "300          280.0       pre      5\n",
      "301          281.0      post      5\n",
      "302          281.0       pre      5\n",
      "303          282.0      post      5\n",
      "304          282.0       pre      5\n",
      "305          283.0      post      5\n",
      "306          283.0       pre      5\n",
      "307          284.0      post      5\n",
      "308          284.0       pre      5\n",
      "309          285.0      post      5\n",
      "310          285.0       pre      5\n",
      "311          286.0      post      5\n",
      "312          286.0       pre      5\n",
      "313          287.0       pre      5\n",
      "314          288.0      post      5\n",
      "315          288.0       pre      5\n",
      "316          289.0       pre      5\n",
      "317          290.0       pre      5\n",
      "318          291.0      post      5\n",
      "319          294.0      post      5\n",
      "320          294.0       pre      5\n",
      "321          295.0      post      5\n",
      "322          295.0       pre      5\n",
      "323          296.0      post      5\n",
      "324          296.0       pre      5\n",
      "325          297.0       pre      5\n",
      "326          298.0       pre      5\n",
      "327          299.0       pre      5\n",
      "328          300.0       pre      5\n",
      "329          301.0      post      5\n",
      "330          301.0       pre      5\n",
      "331          302.0      post      5\n",
      "332          302.0       pre      5\n",
      "333          303.0      post      5\n",
      "334          303.0       pre      5\n",
      "335          304.0      post      5\n",
      "336          304.0       pre      5\n",
      "337          305.0       pre      5\n",
      "338          308.0      post      5\n",
      "339          308.0       pre      5\n",
      "340              1      post      5\n",
      "341              1       pre      5\n",
      "342             10       pre      5\n",
      "343            100       pre     10\n",
      "344            101       pre     10\n",
      "345            102       pre     10\n",
      "346            103       pre     10\n",
      "347            104       pre     10\n",
      "348            105       pre     10\n",
      "349            106       pre     10\n",
      "350             11      post      5\n",
      "351             11       pre      5\n",
      "352             12       pre      4\n",
      "353             13       pre      4\n",
      "354             14       pre      5\n",
      "355             15       pre      5\n",
      "356             16       pre      5\n",
      "357             17      post      5\n",
      "358             17       pre      5\n",
      "359             18       pre      5\n",
      "360             19       pre      5\n",
      "361              2      post      5\n",
      "362              2       pre      5\n",
      "363             20      post      5\n",
      "364             20       pre      5\n",
      "365             21       pre      5\n",
      "366             22       pre      3\n",
      "367             23      post      5\n",
      "368             23       pre      5\n",
      "369             24      post      5\n",
      "370             24       pre      5\n",
      "371             25      post      5\n",
      "372             25       pre      5\n",
      "373             26       pre      4\n",
      "374             27       pre      5\n",
      "375             28       pre      5\n",
      "376             29       pre      5\n",
      "377              3       pre      5\n",
      "378             30       pre      5\n",
      "379             31       pre      3\n",
      "380             32      post      5\n",
      "381             32       pre      5\n",
      "382             33      post      5\n",
      "383             33       pre      5\n",
      "384             34       pre      5\n",
      "385             35       pre      5\n",
      "386             36      post      5\n",
      "387             36       pre      5\n",
      "388             37       pre      5\n",
      "389             38       pre      5\n",
      "390             39       pre      5\n",
      "391              4       pre      5\n",
      "392             40       pre      5\n",
      "393             41      post      5\n",
      "394             41       pre      5\n",
      "395             42      post      5\n",
      "396             42       pre      5\n",
      "397             43      post      5\n",
      "398             43       pre      5\n",
      "399             44      post      5\n",
      "400             44       pre      5\n",
      "401             45       pre      5\n",
      "402             46      post      5\n",
      "403             46       pre      5\n",
      "404             47      post      5\n",
      "405             47       pre      5\n",
      "406             48      post      5\n",
      "407             48       pre      5\n",
      "408             49      post      5\n",
      "409             49       pre      5\n",
      "410              5      post      5\n",
      "411              5       pre      5\n",
      "412             50      post      5\n",
      "413             50       pre      5\n",
      "414             51      post      5\n",
      "415             51       pre      5\n",
      "416             52       pre      5\n",
      "417             53       pre      5\n",
      "418             54       pre      2\n",
      "419             55       pre      4\n",
      "420             56       pre      5\n",
      "421             57       pre      5\n",
      "422             58       pre      5\n",
      "423             59       pre      5\n",
      "424              6      post      5\n",
      "425              6       pre      5\n",
      "426             60       pre      3\n",
      "427             61       pre      3\n",
      "428             62       pre      5\n",
      "429             63       pre      5\n",
      "430             64      post      5\n",
      "431             64       pre      5\n",
      "432             65       pre      5\n",
      "433             66       pre      5\n",
      "434             67      post      5\n",
      "435             67       pre      5\n",
      "436             68      post      5\n",
      "437             68       pre      5\n",
      "438             69      post      5\n",
      "439             69       pre      5\n",
      "440              7      post      5\n",
      "441              7       pre      5\n",
      "442             70      post      5\n",
      "443             70       pre      5\n",
      "444             71      post      5\n",
      "445             71       pre      5\n",
      "446             72       pre      5\n",
      "447             73       pre      5\n",
      "448             74      post      5\n",
      "449             74       pre      5\n",
      "450             75      post      5\n",
      "451             75       pre      5\n",
      "452             76      post      4\n",
      "453             76       pre      5\n",
      "454             77       pre      5\n",
      "455             78       pre      5\n",
      "456             79       pre      5\n",
      "457              8      post      5\n",
      "458              8       pre      5\n",
      "459             80      post      5\n",
      "460             80       pre      5\n",
      "461             81      post      5\n",
      "462             81       pre      5\n",
      "463             82       pre      4\n",
      "464             83       pre      2\n",
      "465             84       pre      5\n",
      "466             85       pre      5\n",
      "467             86       pre      5\n",
      "468             87       pre      1\n",
      "469             88       pre      5\n",
      "470              9       pre      5\n",
      "471             95       pre     10\n",
      "472             96       pre     10\n",
      "473             97       pre     10\n",
      "474             98       pre     10\n",
      "475             99       pre     10\n"
     ]
    }
   ],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "# Group the DataFrame by 'Patient Number' and 'Procedure' and count the occurrences\n",
    "procedure_counts = df.groupby(['Patient Number', 'Procedure']).size().reset_index(name='Count')\n",
    "\n",
    "print(procedure_counts)\n",
    "pd.reset_option(\"display.max_rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/pbs.7679917.pbs/ipykernel_3298172/908500116.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = 0  # Initialize the column with zeros\n",
      "/var/tmp/pbs.7679917.pbs/ipykernel_3298172/908500116.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = 0  # Initialize the column with zeros\n",
      "/var/tmp/pbs.7679917.pbs/ipykernel_3298172/908500116.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = 0  # Initialize the column with zeros\n",
      "/var/tmp/pbs.7679917.pbs/ipykernel_3298172/908500116.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = 0  # Initialize the column with zeros\n",
      "/var/tmp/pbs.7679917.pbs/ipykernel_3298172/908500116.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = 0  # Initialize the column with zeros\n",
      "/var/tmp/pbs.7679917.pbs/ipykernel_3298172/908500116.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = 0  # Initialize the column with zeros\n",
      "/var/tmp/pbs.7679917.pbs/ipykernel_3298172/908500116.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = 0  # Initialize the column with zeros\n",
      "/var/tmp/pbs.7679917.pbs/ipykernel_3298172/908500116.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = 0  # Initialize the column with zeros\n",
      "/var/tmp/pbs.7679917.pbs/ipykernel_3298172/908500116.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = 0  # Initialize the column with zeros\n",
      "/var/tmp/pbs.7679917.pbs/ipykernel_3298172/908500116.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = 0  # Initialize the column with zeros\n",
      "/var/tmp/pbs.7679917.pbs/ipykernel_3298172/908500116.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = 0  # Initialize the column with zeros\n",
      "/var/tmp/pbs.7679917.pbs/ipykernel_3298172/908500116.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = 0  # Initialize the column with zeros\n",
      "/var/tmp/pbs.7679917.pbs/ipykernel_3298172/908500116.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = 0  # Initialize the column with zeros\n",
      "/var/tmp/pbs.7679917.pbs/ipykernel_3298172/908500116.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = 0  # Initialize the column with zeros\n",
      "/var/tmp/pbs.7679917.pbs/ipykernel_3298172/908500116.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = 0  # Initialize the column with zeros\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              File Path Patient Number  \\\n",
      "0     /rds/general/project/fsn-ai-ecg-data/live/afml...             48   \n",
      "1     /rds/general/project/fsn-ai-ecg-data/live/afml...             60   \n",
      "2     /rds/general/project/fsn-ai-ecg-data/live/afml...             15   \n",
      "3     /rds/general/project/fsn-ai-ecg-data/live/afml...             81   \n",
      "4     /rds/general/project/fsn-ai-ecg-data/live/afml...             49   \n",
      "...                                                 ...            ...   \n",
      "2411  /rds/general/project/fsn-ai-ecg-data/live/afml...          196.0   \n",
      "2412  /rds/general/project/fsn-ai-ecg-data/live/afml...          181.0   \n",
      "2413  /rds/general/project/fsn-ai-ecg-data/live/afml...          133.0   \n",
      "2414  /rds/general/project/fsn-ai-ecg-data/live/afml...          208.0   \n",
      "2415  /rds/general/project/fsn-ai-ecg-data/live/afml...          244.0   \n",
      "\n",
      "     Procedure  Channel_I  Channel_aVF  Channel_V1  Channel_V6  \\\n",
      "0         post          1            1           1           1   \n",
      "1          pre          1            1           1           1   \n",
      "2          pre          1            1           1           1   \n",
      "3          pre          1            1           1           1   \n",
      "4         post          1            1           1           1   \n",
      "...        ...        ...          ...         ...         ...   \n",
      "2411      post          1            1           1           1   \n",
      "2412       pre          1            1           1           1   \n",
      "2413       pre          1            1           1           1   \n",
      "2414      post          1            1           1           1   \n",
      "2415       pre          1            1           1           1   \n",
      "\n",
      "      Channel_CS 1-2  Channel_CS 3-4  Channel_CS 5-6  ...  Channel_Map D  \\\n",
      "0                  1               1               0  ...              0   \n",
      "1                  1               1               0  ...              0   \n",
      "2                  1               1               1  ...              0   \n",
      "3                  1               1               1  ...              0   \n",
      "4                  1               1               1  ...              0   \n",
      "...              ...             ...             ...  ...            ...   \n",
      "2411               0               1               1  ...              0   \n",
      "2412               1               1               1  ...              0   \n",
      "2413               1               1               1  ...              0   \n",
      "2414               1               1               1  ...              0   \n",
      "2415               1               1               1  ...              0   \n",
      "\n",
      "      Channel_Map P  Channel_cs 5-6  Channel_decanav 1-2  Channel_decanav 3-4  \\\n",
      "0                 0               0                    0                    0   \n",
      "1                 0               0                    0                    0   \n",
      "2                 0               0                    0                    0   \n",
      "3                 0               0                    0                    0   \n",
      "4                 0               0                    0                    0   \n",
      "...             ...             ...                  ...                  ...   \n",
      "2411              0               0                    0                    0   \n",
      "2412              0               0                    0                    0   \n",
      "2413              0               0                    0                    0   \n",
      "2414              0               0                    0                    0   \n",
      "2415              0               0                    0                    0   \n",
      "\n",
      "      Channel_decanav 5-6  Channel_decanav 7-8  Channel_decanav 9-10  \\\n",
      "0                       0                    0                     0   \n",
      "1                       0                    0                     0   \n",
      "2                       0                    0                     0   \n",
      "3                       0                    0                     0   \n",
      "4                       0                    0                     0   \n",
      "...                   ...                  ...                   ...   \n",
      "2411                    0                    0                     0   \n",
      "2412                    0                    0                     0   \n",
      "2413                    0                    0                     0   \n",
      "2414                    0                    0                     0   \n",
      "2415                    0                    0                     0   \n",
      "\n",
      "      Channel_HIS d  Channel_HIS P  \n",
      "0                 0              0  \n",
      "1                 0              0  \n",
      "2                 0              0  \n",
      "3                 0              0  \n",
      "4                 0              0  \n",
      "...             ...            ...  \n",
      "2411              0              0  \n",
      "2412              0              0  \n",
      "2413              0              0  \n",
      "2414              0              0  \n",
      "2415              0              0  \n",
      "\n",
      "[2415 rows x 117 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Rest of your code...\n",
    "\n",
    "# Iterate through the DataFrame rows\n",
    "for index, row in df.iterrows():\n",
    "    filepath = row['File Path']\n",
    "    txt_file = TxtFile(filepath)\n",
    "\n",
    "    # Iterate through the channels in the TxtFile object\n",
    "    for channel in txt_file.channels:\n",
    "        # Create a column name based on the channel\n",
    "        column_name = f\"Channel_{channel}\"\n",
    "        \n",
    "        # Add a new column to the DataFrame if it doesn't exist\n",
    "        if column_name not in df.columns:\n",
    "            df[column_name] = 0  # Initialize the column with zeros\n",
    "        \n",
    "        # Set the value to 1 if the channel is present in the TxtFile channels\n",
    "        if channel in txt_file.channels:\n",
    "            df.at[index, column_name] = 1\n",
    "\n",
    "print(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving csv\n",
    "df.to_csv(\"output.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with no zeros:\n",
      "Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "columns_no_zeros = df.loc[:, 'Channel_I':].columns[df.loc[:, 'Channel_I':].all()]\n",
    "print(\"Columns with no zeros:\")\n",
    "print(columns_no_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of zeros in each channel column:\n",
      "Channel_I                1.656315\n",
      "Channel_aVF              1.449275\n",
      "Channel_V1               5.962733\n",
      "Channel_V6               3.478261\n",
      "Channel_CS 1-2          10.310559\n",
      "Channel_CS 3-4           4.720497\n",
      "Channel_CS 5-6           8.695652\n",
      "Channel_CS 7-8           8.902692\n",
      "Channel_CS 9-10          9.109731\n",
      "Channel_CS 1,2          98.385093\n",
      "Channel_CS 3,4          98.923395\n",
      "Channel_CS 5,6          98.923395\n",
      "Channel_CS 7,8          99.130435\n",
      "Channel_CS 9,10         99.130435\n",
      "Channel_II              88.902692\n",
      "Channel_V2              92.380952\n",
      "Channel_CS 3, 4         99.461698\n",
      "Channel_CS 5 , 6        99.461698\n",
      "Channel_CS 7 ,8         99.461698\n",
      "Channel_CS 9, 10        99.461698\n",
      "Channel_III             95.031056\n",
      "Channel_aVR             96.480331\n",
      "Channel_aVL             96.480331\n",
      "Channel_V3              96.687371\n",
      "Channel_V4              96.687371\n",
      "Channel_V5              96.107660\n",
      "Channel_Mapd            43.064182\n",
      "Channel_Mapp            43.064182\n",
      "Channel_CS1-2           94.409938\n",
      "Channel_HRA p           97.515528\n",
      "Channel_PV 1-2          99.171843\n",
      "Channel_PV 3-4          99.171843\n",
      "Channel_PV 5-6          99.171843\n",
      "Channel_PV 7-8          99.171843\n",
      "Channel_PV 9-10         99.171843\n",
      "Channel_PV 11-12        99.171843\n",
      "Channel_PV 13-14        99.171843\n",
      "Channel_PV 15-16        99.171843\n",
      "Channel_PV 17-18        99.171843\n",
      "Channel_PV 19-20        98.964803\n",
      "Channel_HRA d           98.757764\n",
      "Channel_HD 1-2          99.585921\n",
      "Channel_HD 2-3          99.585921\n",
      "Channel_HD 3-4          99.585921\n",
      "Channel_HD 4-5          99.585921\n",
      "Channel_HD 5-6          99.585921\n",
      "Channel_HD 6-7          99.585921\n",
      "Channel_HD 7-8          99.585921\n",
      "Channel_HD 8-9          99.585921\n",
      "Channel_HD 9-10         99.585921\n",
      "Channel_HD 10-11        99.585921\n",
      "Channel_HD 11-12        99.585921\n",
      "Channel_HD 12-13        99.585921\n",
      "Channel_HD 13-14        99.585921\n",
      "Channel_HD 14-15        99.585921\n",
      "Channel_HD 15-16        99.585921\n",
      "Channel_HD 16-17        99.585921\n",
      "Channel_HD 17-18        99.585921\n",
      "Channel_HD 18-19        99.585921\n",
      "Channel_HD 19-20        99.585921\n",
      "Channel_RVp             98.178054\n",
      "Channel_Achieve 1-2     99.585921\n",
      "Channel_Achieve 2-3     99.585921\n",
      "Channel_Achieve 3-4     99.585921\n",
      "Channel_Achieve 4-5     99.585921\n",
      "Channel_Achieve 5-6     99.585921\n",
      "Channel_Achieve 6-7     99.585921\n",
      "Channel_Achieve 7-8     99.585921\n",
      "Channel_PV 1,2          99.171843\n",
      "Channel_PV 3,4          99.171843\n",
      "Channel_PV 5,6          99.171843\n",
      "Channel_PV 7,8          99.171843\n",
      "Channel_PV 9,10         99.171843\n",
      "Channel_PV 11,12        99.171843\n",
      "Channel_PV 13,14        99.171843\n",
      "Channel_PV 15,16        99.171843\n",
      "Channel_PV 17,18        99.171843\n",
      "Channel_PV 19, 20       99.171843\n",
      "Channel_HALO 1-2        99.751553\n",
      "Channel_HALO 3-4        99.751553\n",
      "Channel_HALO 5-6        99.751553\n",
      "Channel_HALO 7-8        99.751553\n",
      "Channel_HALO 9-10       99.751553\n",
      "Channel_HALO 11-12      99.751553\n",
      "Channel_HALO 13-14      99.751553\n",
      "Channel_HALO 15-16      99.751553\n",
      "Channel_HALO 17-18      99.751553\n",
      "Channel_HALO 19-20      99.751553\n",
      "Channel_PV 19,20        99.792961\n",
      "Channel_LA 3,4          99.792961\n",
      "Channel_PV 2-3          99.585921\n",
      "Channel_PV 4-5          99.585921\n",
      "Channel_PV 6-7          99.585921\n",
      "Channel_PV 8-9          99.585921\n",
      "Channel_PV 10-11        99.585921\n",
      "Channel_PV 12-13        99.585921\n",
      "Channel_PV 14-15        99.585921\n",
      "Channel_PV 16-17        99.585921\n",
      "Channel_PV 18-19        99.585921\n",
      "Channel_RVd             99.006211\n",
      "Channel_HRAp            98.799172\n",
      "Channel_HISd            99.585921\n",
      "Channel_HISp            99.213251\n",
      "Channel_HRAd            99.792961\n",
      "Channel_Map D           99.792961\n",
      "Channel_Map P           99.792961\n",
      "Channel_cs 5-6          99.792961\n",
      "Channel_decanav 1-2     99.792961\n",
      "Channel_decanav 3-4     99.792961\n",
      "Channel_decanav 5-6     99.792961\n",
      "Channel_decanav 7-8     99.792961\n",
      "Channel_decanav 9-10    99.792961\n",
      "Channel_HIS d           99.420290\n",
      "Channel_HIS P           99.792961\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "#caclulating percentage of zero in channel collumns \n",
    "channel_columns = df.loc[:, 'Channel_I':]\n",
    "\n",
    "zero_percentages = channel_columns.eq(0).mean() * 100\n",
    "print(\"Percentage of zeros in each channel column:\")\n",
    "print(zero_percentages)\n",
    "pd.reset_option(\"display.max_rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in the smaples per channel of each txt file to make sure we have 60,000 milliseconds of data for each txt file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_samples_per_channel(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith(\"Samples per channel\"):\n",
    "                samples_per_channel = line.split(\":\")[1].strip()\n",
    "                return int(samples_per_channel)\n",
    "    return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              File Path Patient Number  \\\n",
      "0     /rds/general/project/fsn-ai-ecg-data/live/afml...             48   \n",
      "1     /rds/general/project/fsn-ai-ecg-data/live/afml...             60   \n",
      "2     /rds/general/project/fsn-ai-ecg-data/live/afml...             15   \n",
      "3     /rds/general/project/fsn-ai-ecg-data/live/afml...             81   \n",
      "4     /rds/general/project/fsn-ai-ecg-data/live/afml...             49   \n",
      "...                                                 ...            ...   \n",
      "2411  /rds/general/project/fsn-ai-ecg-data/live/afml...          196.0   \n",
      "2412  /rds/general/project/fsn-ai-ecg-data/live/afml...          181.0   \n",
      "2413  /rds/general/project/fsn-ai-ecg-data/live/afml...          133.0   \n",
      "2414  /rds/general/project/fsn-ai-ecg-data/live/afml...          208.0   \n",
      "2415  /rds/general/project/fsn-ai-ecg-data/live/afml...          244.0   \n",
      "\n",
      "     Procedure  Channel_I  Channel_aVF  Channel_V1  Channel_V6  \\\n",
      "0         post          1            1           1           1   \n",
      "1          pre          1            1           1           1   \n",
      "2          pre          1            1           1           1   \n",
      "3          pre          1            1           1           1   \n",
      "4         post          1            1           1           1   \n",
      "...        ...        ...          ...         ...         ...   \n",
      "2411      post          1            1           1           1   \n",
      "2412       pre          1            1           1           1   \n",
      "2413       pre          1            1           1           1   \n",
      "2414      post          1            1           1           1   \n",
      "2415       pre          1            1           1           1   \n",
      "\n",
      "      Channel_CS 1-2  Channel_CS 3-4  Channel_CS 5-6  ...  Channel_Map P  \\\n",
      "0                  1               1               0  ...              0   \n",
      "1                  1               1               0  ...              0   \n",
      "2                  1               1               1  ...              0   \n",
      "3                  1               1               1  ...              0   \n",
      "4                  1               1               1  ...              0   \n",
      "...              ...             ...             ...  ...            ...   \n",
      "2411               0               1               1  ...              0   \n",
      "2412               1               1               1  ...              0   \n",
      "2413               1               1               1  ...              0   \n",
      "2414               1               1               1  ...              0   \n",
      "2415               1               1               1  ...              0   \n",
      "\n",
      "      Channel_cs 5-6  Channel_decanav 1-2  Channel_decanav 3-4  \\\n",
      "0                  0                    0                    0   \n",
      "1                  0                    0                    0   \n",
      "2                  0                    0                    0   \n",
      "3                  0                    0                    0   \n",
      "4                  0                    0                    0   \n",
      "...              ...                  ...                  ...   \n",
      "2411               0                    0                    0   \n",
      "2412               0                    0                    0   \n",
      "2413               0                    0                    0   \n",
      "2414               0                    0                    0   \n",
      "2415               0                    0                    0   \n",
      "\n",
      "      Channel_decanav 5-6  Channel_decanav 7-8  Channel_decanav 9-10  \\\n",
      "0                       0                    0                     0   \n",
      "1                       0                    0                     0   \n",
      "2                       0                    0                     0   \n",
      "3                       0                    0                     0   \n",
      "4                       0                    0                     0   \n",
      "...                   ...                  ...                   ...   \n",
      "2411                    0                    0                     0   \n",
      "2412                    0                    0                     0   \n",
      "2413                    0                    0                     0   \n",
      "2414                    0                    0                     0   \n",
      "2415                    0                    0                     0   \n",
      "\n",
      "      Channel_HIS d  Channel_HIS P  Sample Channels  \n",
      "0                 0              0            60000  \n",
      "1                 0              0            60000  \n",
      "2                 0              0            60000  \n",
      "3                 0              0            60000  \n",
      "4                 0              0            60000  \n",
      "...             ...            ...              ...  \n",
      "2411              0              0            60000  \n",
      "2412              0              0            60000  \n",
      "2413              0              0            60000  \n",
      "2414              0              0            60000  \n",
      "2415              0              0            60000  \n",
      "\n",
      "[2415 rows x 118 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/pbs.7679917.pbs/ipykernel_3298172/2024681533.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Sample Channels'] = df['File Path'].apply(read_samples_per_channel)\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the DataFrame and add a new column \"Sample Channels\"\n",
    "df['Sample Channels'] = df['File Path'].apply(read_samples_per_channel)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows without 60,000 sample channels:\n",
      "                                              File Path Patient Number  \\\n",
      "31    /rds/general/project/fsn-ai-ecg-data/live/afml...             83   \n",
      "250   /rds/general/project/fsn-ai-ecg-data/live/afml...             26   \n",
      "449   /rds/general/project/fsn-ai-ecg-data/live/afml...             12   \n",
      "533   /rds/general/project/fsn-ai-ecg-data/live/afml...             61   \n",
      "585   /rds/general/project/fsn-ai-ecg-data/live/afml...             48   \n",
      "...                                                 ...            ...   \n",
      "2335  /rds/general/project/fsn-ai-ecg-data/live/afml...          250.0   \n",
      "2349  /rds/general/project/fsn-ai-ecg-data/live/afml...          298.0   \n",
      "2350  /rds/general/project/fsn-ai-ecg-data/live/afml...          118.0   \n",
      "2356  /rds/general/project/fsn-ai-ecg-data/live/afml...          228.0   \n",
      "2380  /rds/general/project/fsn-ai-ecg-data/live/afml...          297.0   \n",
      "\n",
      "     Procedure  Channel_I  Channel_aVF  Channel_V1  Channel_V6  \\\n",
      "31         pre          1            1           1           1   \n",
      "250        pre          1            1           1           1   \n",
      "449        pre          1            1           1           1   \n",
      "533        pre          1            1           1           1   \n",
      "585        pre          1            1           1           1   \n",
      "...        ...        ...          ...         ...         ...   \n",
      "2335       pre          1            1           1           1   \n",
      "2349       pre          1            1           1           1   \n",
      "2350       pre          1            1           1           1   \n",
      "2356       pre          1            1           1           1   \n",
      "2380       pre          1            1           1           1   \n",
      "\n",
      "      Channel_CS 1-2  Channel_CS 3-4  Channel_CS 5-6  ...  Channel_Map P  \\\n",
      "31                 0               0               0  ...              0   \n",
      "250                1               1               1  ...              0   \n",
      "449                1               1               1  ...              0   \n",
      "533                1               1               1  ...              0   \n",
      "585                1               1               0  ...              0   \n",
      "...              ...             ...             ...  ...            ...   \n",
      "2335               1               1               1  ...              0   \n",
      "2349               1               1               1  ...              0   \n",
      "2350               1               1               1  ...              0   \n",
      "2356               1               1               1  ...              0   \n",
      "2380               1               1               1  ...              0   \n",
      "\n",
      "      Channel_cs 5-6  Channel_decanav 1-2  Channel_decanav 3-4  \\\n",
      "31                 0                    0                    0   \n",
      "250                0                    0                    0   \n",
      "449                0                    0                    0   \n",
      "533                0                    0                    0   \n",
      "585                0                    0                    0   \n",
      "...              ...                  ...                  ...   \n",
      "2335               0                    0                    0   \n",
      "2349               0                    0                    0   \n",
      "2350               0                    0                    0   \n",
      "2356               0                    0                    0   \n",
      "2380               0                    0                    0   \n",
      "\n",
      "      Channel_decanav 5-6  Channel_decanav 7-8  Channel_decanav 9-10  \\\n",
      "31                      0                    0                     0   \n",
      "250                     0                    0                     0   \n",
      "449                     0                    0                     0   \n",
      "533                     0                    0                     0   \n",
      "585                     0                    0                     0   \n",
      "...                   ...                  ...                   ...   \n",
      "2335                    0                    0                     0   \n",
      "2349                    0                    0                     0   \n",
      "2350                    0                    0                     0   \n",
      "2356                    0                    0                     0   \n",
      "2380                    0                    0                     0   \n",
      "\n",
      "      Channel_HIS d  Channel_HIS P  Sample Channels  \n",
      "31                0              0            59040  \n",
      "250               0              0            59040  \n",
      "449               0              0            59040  \n",
      "533               0              0            59040  \n",
      "585               0              0            59904  \n",
      "...             ...            ...              ...  \n",
      "2335              0              0            58400  \n",
      "2349              0              0            58400  \n",
      "2350              0              0            59552  \n",
      "2356              0              0            58144  \n",
      "2380              0              0            58336  \n",
      "\n",
      "[68 rows x 118 columns]\n",
      "Number of rows without 60,000 sample channels: 68\n",
      "Invalid rows removed.\n",
      "(2347, 118)\n"
     ]
    }
   ],
   "source": [
    "# Check for rows with sample channels not equal to 60,000\n",
    "invalid_rows = df[df['Sample Channels'] != 60000]\n",
    "\n",
    "# Check if there are any invalid rows\n",
    "if not invalid_rows.empty:\n",
    "    print(\"Rows without 60,000 sample channels:\")\n",
    "    print(invalid_rows)\n",
    "    num_invalid_rows = len(invalid_rows)\n",
    "    print(\"Number of rows without 60,000 sample channels:\", num_invalid_rows)\n",
    "    \n",
    "    # Remove the invalid rows from the DataFrame\n",
    "    df = df.drop(invalid_rows.index)\n",
    "\n",
    "    print(\"Invalid rows removed.\")\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(df.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the final df \n",
    "df.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option(\"display.max_rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "class TxtFile:\n",
    "    def __init__(self, filepath, verbose=False):\n",
    "        self.filepath = filepath\n",
    "        self.channels, self.sample_freq, self.data = self.load_file()\n",
    "        if verbose: print(\"Channels: {}\".format(self.channels))\n",
    "\n",
    "    def load_file(self):\n",
    "        with open(self.filepath) as f:\n",
    "            channels, sample_freq = self.load_channels(f)\n",
    "            _ = self._read_until(f, \"[Data]\")\n",
    "            data = f.read()\n",
    "            data = pd.read_table(StringIO(data), names=channels, sep=',')\n",
    "            # data = self.filter_data(data)\n",
    "            return channels, sample_freq, data\n",
    "\n",
    "    def load_channels(self, file):\n",
    "        channels = []\n",
    "        line = self._read_until(file, \"Channels exported:\")\n",
    "        sample_freq = int(self._read_until(file, \"Sample Rate\").rsplit(' ', 1)[-1].rsplit('Hz')[0])\n",
    "        n_channels = int(line.split(' ')[-1])\n",
    "        for n_channel in range(n_channels):\n",
    "            line = self._read_until(file, \"Label:\")\n",
    "            channel_name = line.replace('Label: ', '').rstrip()\n",
    "            channels.append(channel_name)\n",
    "        return channels, sample_freq\n",
    "\n",
    "    @staticmethod\n",
    "    def _read_until(file, string):\n",
    "        line = file.readline()\n",
    "        while string not in line:\n",
    "            line = file.readline()\n",
    "        return line\n",
    "\n",
    "    def get_data_shape(self):\n",
    "        return self.data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "csv with all 60,000 samples exactly widled down to 2347 samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DataShape\"] = df[\"File Path\"].apply(lambda x: TxtFile(x).get_data_shape())\n",
    "df.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2346, 119)\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with data shape other than (60000, *)\n",
    "df = df[df[\"DataShape\"].apply(lambda shape: shape[0] == 60000)]\n",
    "print(df.shape)\n",
    "# Save the updated DataFrame to a CSV file\n",
    "df.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with a value of 1 in all channel columns: 2070\n",
      "New DataFrame with the specified channel columns and additional columns:\n",
      "      Channel_I  Channel_aVF  Channel_V1  Channel_V6  Channel_CS 3-4  \\\n",
      "0             1            1           1           1               1   \n",
      "1             1            1           1           1               1   \n",
      "2             1            1           1           1               1   \n",
      "3             1            1           1           1               1   \n",
      "4             1            1           1           1               1   \n",
      "...         ...          ...         ...         ...             ...   \n",
      "2411          1            1           1           1               1   \n",
      "2412          1            1           1           1               1   \n",
      "2413          1            1           1           1               1   \n",
      "2414          1            1           1           1               1   \n",
      "2415          1            1           1           1               1   \n",
      "\n",
      "                                              File Path Patient Number  \\\n",
      "0     /rds/general/project/fsn-ai-ecg-data/live/afml...             48   \n",
      "1     /rds/general/project/fsn-ai-ecg-data/live/afml...             60   \n",
      "2     /rds/general/project/fsn-ai-ecg-data/live/afml...             15   \n",
      "3     /rds/general/project/fsn-ai-ecg-data/live/afml...             81   \n",
      "4     /rds/general/project/fsn-ai-ecg-data/live/afml...             49   \n",
      "...                                                 ...            ...   \n",
      "2411  /rds/general/project/fsn-ai-ecg-data/live/afml...          196.0   \n",
      "2412  /rds/general/project/fsn-ai-ecg-data/live/afml...          181.0   \n",
      "2413  /rds/general/project/fsn-ai-ecg-data/live/afml...          133.0   \n",
      "2414  /rds/general/project/fsn-ai-ecg-data/live/afml...          208.0   \n",
      "2415  /rds/general/project/fsn-ai-ecg-data/live/afml...          244.0   \n",
      "\n",
      "     Procedure  \n",
      "0         post  \n",
      "1          pre  \n",
      "2          pre  \n",
      "3          pre  \n",
      "4         post  \n",
      "...        ...  \n",
      "2411      post  \n",
      "2412       pre  \n",
      "2413       pre  \n",
      "2414      post  \n",
      "2415       pre  \n",
      "\n",
      "[2070 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "#creating a final_df with 1 with each of these channels \n",
    "\n",
    "# Specify the channel columns\n",
    "channel_columns = ['Channel_I', 'Channel_aVF', 'Channel_V1', 'Channel_V6', 'Channel_CS 3-4']\n",
    "\n",
    "# Filter rows that have a value of 1 in all channel columns\n",
    "rows_with_ones = df[(df[channel_columns] == 1).all(axis=1)]\n",
    "\n",
    "# Create a new DataFrame with the specified channel columns and additional columns\n",
    "additional_columns = ['File Path', 'Patient Number', 'Procedure']\n",
    "final_df = rows_with_ones[channel_columns + additional_columns].copy()\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(\"Number of rows with a value of 1 in all channel columns:\", len(final_df))\n",
    "print(\"New DataFrame with the specified channel columns and additional columns:\")\n",
    "print(final_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique patients: 259\n",
      "\n",
      "number of ecg samples per patient:\n",
      "125.0    10\n",
      "204.0    10\n",
      "288.0    10\n",
      "122.0    10\n",
      "183.0    10\n",
      "294.0    10\n",
      "131.0    10\n",
      "158.0    10\n",
      "253.0    10\n",
      "249.0    10\n",
      "184.0    10\n",
      "185.0    10\n",
      "168.0    10\n",
      "187.0    10\n",
      "236.0    10\n",
      "181.0    10\n",
      "110.0    10\n",
      "264.0    10\n",
      "135.0    10\n",
      "140.0    10\n",
      "157.0    10\n",
      "265.0    10\n",
      "201.0    10\n",
      "42       10\n",
      "238.0    10\n",
      "152.0    10\n",
      "243.0    10\n",
      "256.0    10\n",
      "111.0    10\n",
      "145.0    10\n",
      "143.0    10\n",
      "138.0    10\n",
      "191.0    10\n",
      "120.0    10\n",
      "193.0    10\n",
      "200.0    10\n",
      "303.0    10\n",
      "217.0    10\n",
      "192.0    10\n",
      "132.0    10\n",
      "308.0    10\n",
      "167.0    10\n",
      "170.0    10\n",
      "141.0    10\n",
      "252.0    10\n",
      "199.0    10\n",
      "237.0    10\n",
      "266.0    10\n",
      "263.0    10\n",
      "283.0    10\n",
      "284.0    10\n",
      "159.0    10\n",
      "211.0    10\n",
      "280.0    10\n",
      "129.0    10\n",
      "286.0    10\n",
      "166.0    10\n",
      "206.0    10\n",
      "285.0    10\n",
      "208.0    10\n",
      "223.0    10\n",
      "156.0    10\n",
      "304.0    10\n",
      "257.0    10\n",
      "224.0    10\n",
      "258.0    10\n",
      "262.0    10\n",
      "1        10\n",
      "70       10\n",
      "126.0    10\n",
      "188.0    10\n",
      "51       10\n",
      "7        10\n",
      "241.0    10\n",
      "151.0    10\n",
      "33       10\n",
      "20       10\n",
      "155.0    10\n",
      "64       10\n",
      "24       10\n",
      "46       10\n",
      "74       10\n",
      "68       10\n",
      "11       10\n",
      "44       10\n",
      "47       10\n",
      "205.0    10\n",
      "123.0    10\n",
      "232.0    10\n",
      "218.0    10\n",
      "23       10\n",
      "5        10\n",
      "36       10\n",
      "8        10\n",
      "49       10\n",
      "81       10\n",
      "108.0    10\n",
      "25       10\n",
      "41       10\n",
      "6        10\n",
      "114.0    10\n",
      "248.0    10\n",
      "142.0    10\n",
      "69       10\n",
      "71       10\n",
      "197.0    10\n",
      "2        10\n",
      "233.0    10\n",
      "50       10\n",
      "139.0    10\n",
      "240.0    10\n",
      "43       10\n",
      "32       10\n",
      "115.0    10\n",
      "75       10\n",
      "112.0    10\n",
      "220.0    10\n",
      "17       10\n",
      "275.0    10\n",
      "242.0    10\n",
      "254.0    10\n",
      "244.0    10\n",
      "251.0    10\n",
      "153.0    10\n",
      "229.0     9\n",
      "202.0     9\n",
      "116.0     9\n",
      "113.0     9\n",
      "190.0     9\n",
      "302.0     9\n",
      "163.0     9\n",
      "296.0     9\n",
      "228.0     9\n",
      "179.0     9\n",
      "203.0     9\n",
      "226.0     9\n",
      "260.0     9\n",
      "247.0     9\n",
      "146.0     9\n",
      "246.0     9\n",
      "162.0     9\n",
      "48        9\n",
      "119.0     9\n",
      "177.0     9\n",
      "150.0     9\n",
      "80        9\n",
      "134.0     9\n",
      "245.0     9\n",
      "76        9\n",
      "235.0     9\n",
      "295.0     9\n",
      "196.0     9\n",
      "301.0     9\n",
      "175.0     9\n",
      "209.0     9\n",
      "127.0     9\n",
      "144.0     9\n",
      "250.0     9\n",
      "133.0     9\n",
      "160.0     9\n",
      "118.0     9\n",
      "148.0     9\n",
      "194.0     9\n",
      "165.0     9\n",
      "147.0     9\n",
      "121.0     9\n",
      "239.0     9\n",
      "207.0     8\n",
      "222.0     8\n",
      "219.0     8\n",
      "124.0     8\n",
      "225.0     7\n",
      "270.0     6\n",
      "182.0     5\n",
      "213.0     5\n",
      "277.0     5\n",
      "300.0     5\n",
      "269.0     5\n",
      "305.0     5\n",
      "227.0     5\n",
      "198.0     5\n",
      "287.0     5\n",
      "164.0     5\n",
      "272.0     5\n",
      "290.0     5\n",
      "289.0     5\n",
      "299.0     5\n",
      "273.0     5\n",
      "261.0     5\n",
      "35        5\n",
      "29        5\n",
      "9         5\n",
      "88        5\n",
      "4         5\n",
      "14        5\n",
      "86        5\n",
      "34        5\n",
      "65        5\n",
      "27        5\n",
      "85        5\n",
      "37        5\n",
      "3         5\n",
      "19        5\n",
      "28        5\n",
      "73        5\n",
      "58        5\n",
      "53        5\n",
      "52        5\n",
      "56        5\n",
      "16        5\n",
      "10        5\n",
      "78        5\n",
      "30        5\n",
      "59        5\n",
      "57        5\n",
      "39        5\n",
      "15        5\n",
      "72        5\n",
      "66        5\n",
      "40        5\n",
      "101       5\n",
      "171.0     5\n",
      "267.0     5\n",
      "291.0     5\n",
      "234.0     5\n",
      "106       5\n",
      "95        5\n",
      "103       5\n",
      "96        5\n",
      "102       5\n",
      "105       5\n",
      "100       5\n",
      "104       5\n",
      "97        5\n",
      "63        5\n",
      "62        5\n",
      "79        5\n",
      "77        5\n",
      "18        5\n",
      "38        5\n",
      "274.0     4\n",
      "231.0     4\n",
      "298.0     4\n",
      "259.0     4\n",
      "55        4\n",
      "13        4\n",
      "82        4\n",
      "297.0     4\n",
      "276.0     4\n",
      "98        4\n",
      "271.0     4\n",
      "26        3\n",
      "60        3\n",
      "31        3\n",
      "12        3\n",
      "61        2\n",
      "54        2\n",
      "255.0     2\n",
      "87        1\n",
      "Name: Patient Number, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#checking pateint sample counts\n",
    "#looking at the counts of patients and how many ecg samples have been taken \n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "# Count unique patients and their frequencies\n",
    "unique_patients = final_df[\"Patient Number\"].nunique()\n",
    "patient_frequencies = final_df[\"Patient Number\"].value_counts()\n",
    "\n",
    "print(\"Number of unique patients:\", unique_patients)\n",
    "print(\"\\nnumber of ecg samples per patient:\")\n",
    "print(patient_frequencies)\n",
    "pd.reset_option(\"display.max_rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option(\"display.max_rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique patients: 259\n",
      "\n",
      "Number of ECG samples per patient:\n",
      "125.0    10\n",
      "204.0    10\n",
      "288.0    10\n",
      "122.0    10\n",
      "183.0    10\n",
      "         ..\n",
      "12        3\n",
      "61        2\n",
      "54        2\n",
      "255.0     2\n",
      "87        1\n",
      "Name: Patient Number, Length: 259, dtype: int64\n",
      "\n",
      "70% of patients (1438 samples):\n",
      "289.0     5\n",
      "193.0    10\n",
      "273.0     5\n",
      "79        5\n",
      "66        5\n",
      "         ..\n",
      "127.0     9\n",
      "114.0    10\n",
      "202.0     9\n",
      "42       10\n",
      "116.0     9\n",
      "Name: Patient Number, Length: 181, dtype: int64\n",
      "\n",
      "20% of patients (428 samples):\n",
      "143.0    10\n",
      "55        4\n",
      "162.0     9\n",
      "184.0    10\n",
      "287.0     5\n",
      "118.0     9\n",
      "88        5\n",
      "200.0    10\n",
      "248.0    10\n",
      "187.0    10\n",
      "122.0    10\n",
      "203.0     9\n",
      "179.0     9\n",
      "8        10\n",
      "129.0    10\n",
      "198.0     5\n",
      "32       10\n",
      "43       10\n",
      "51       10\n",
      "79        5\n",
      "148.0     9\n",
      "62        5\n",
      "167.0    10\n",
      "97        5\n",
      "44       10\n",
      "16        5\n",
      "163.0     9\n",
      "225.0     7\n",
      "98        4\n",
      "139.0    10\n",
      "258.0    10\n",
      "75       10\n",
      "238.0    10\n",
      "262.0    10\n",
      "26        3\n",
      "57        5\n",
      "157.0    10\n",
      "111.0    10\n",
      "280.0    10\n",
      "119.0     9\n",
      "151.0    10\n",
      "190.0     9\n",
      "102       5\n",
      "96        5\n",
      "103       5\n",
      "42       10\n",
      "182.0     5\n",
      "245.0     9\n",
      "236.0    10\n",
      "145.0    10\n",
      "133.0     9\n",
      "135.0    10\n",
      "Name: Patient Number, dtype: int64\n",
      "\n",
      "10% of patients (205 samples):\n",
      "110.0    10\n",
      "138.0    10\n",
      "298.0     4\n",
      "1        10\n",
      "156.0    10\n",
      "30        5\n",
      "131.0    10\n",
      "275.0    10\n",
      "236.0    10\n",
      "177.0     9\n",
      "192.0    10\n",
      "246.0     9\n",
      "80        9\n",
      "291.0     5\n",
      "42       10\n",
      "95        5\n",
      "96        5\n",
      "69       10\n",
      "272.0     5\n",
      "18        5\n",
      "122.0    10\n",
      "4         5\n",
      "10        5\n",
      "251.0    10\n",
      "102       5\n",
      "190.0     9\n",
      "Name: Patient Number, dtype: int64\n",
      "\n",
      "List of patients in the 70% split:\n",
      "[289.0, 193.0, 273.0, '79', '66', 272.0, 219.0, 256.0, '80', '35', '87', 304.0, 183.0, '100', 252.0, '78', '60', 159.0, 209.0, '13', 241.0, '6', '9', 234.0, '104', 232.0, 192.0, '38', 163.0, '53', '1', 168.0, '8', '64', '47', 269.0, 132.0, '14', '26', '57', 276.0, '97', 254.0, 138.0, 135.0, '96', 124.0, 140.0, 274.0, 285.0, 120.0, '2', 200.0, 185.0, 145.0, 125.0, 148.0, '32', 251.0, 301.0, 300.0, 133.0, 142.0, 111.0, 110.0, 146.0, '54', 266.0, '31', 308.0, '23', '59', 126.0, 228.0, '4', 240.0, 287.0, 198.0, '75', 208.0, '5', 245.0, 290.0, '98', '81', 275.0, 236.0, 147.0, 182.0, 265.0, 218.0, 166.0, 171.0, 286.0, '17', '44', 280.0, 153.0, 299.0, 164.0, 233.0, 270.0, 170.0, 264.0, 294.0, '29', 220.0, 263.0, 242.0, '52', 262.0, 188.0, '95', 298.0, 113.0, 225.0, 129.0, '55', '41', 162.0, 165.0, '50', 160.0, '25', 271.0, '68', '85', 197.0, '15', '49', '10', '74', 229.0, 207.0, 222.0, 187.0, '103', 213.0, 199.0, '76', '69', 303.0, '56', 231.0, '58', 295.0, '33', 255.0, 291.0, 297.0, '65', 288.0, '86', 131.0, '72', '3', 155.0, 115.0, 258.0, 196.0, 118.0, 237.0, 151.0, '36', 261.0, 123.0, 177.0, 227.0, 184.0, 194.0, 191.0, 260.0, 239.0, '11', '19', 121.0, 127.0, 114.0, 202.0, '42', 116.0]\n",
      "\n",
      "List of patients in the 20% split:\n",
      "[143.0, '55', 162.0, 184.0, 287.0, 118.0, '88', 200.0, 248.0, 187.0, 122.0, 203.0, 179.0, '8', 129.0, 198.0, '32', '43', '51', '79', 148.0, '62', 167.0, '97', '44', '16', 163.0, 225.0, '98', 139.0, 258.0, '75', 238.0, 262.0, '26', '57', 157.0, 111.0, 280.0, 119.0, 151.0, 190.0, '102', '96', '103', '42', 182.0, 245.0, 236.0, 145.0, 133.0, 135.0]\n",
      "\n",
      "List of patients in the 10% split:\n",
      "[110.0, 138.0, 298.0, '1', 156.0, '30', 131.0, 275.0, 236.0, 177.0, 192.0, 246.0, '80', 291.0, '42', '95', '96', '69', 272.0, '18', 122.0, '4', '10', 251.0, '102', 190.0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Count unique patients and their frequencies\n",
    "unique_patients = final_df[\"Patient Number\"].nunique()\n",
    "patient_frequencies = final_df[\"Patient Number\"].value_counts()\n",
    "\n",
    "print(\"Number of unique patients:\", unique_patients)\n",
    "print(\"\\nNumber of ECG samples per patient:\")\n",
    "print(patient_frequencies)\n",
    "\n",
    "# Split the frequencies based on percentages\n",
    "split_frequencies = [\n",
    "    patient_frequencies.sample(frac=0.7, random_state=1),\n",
    "    patient_frequencies.sample(frac=0.2, random_state=2),\n",
    "    patient_frequencies.sample(frac=0.1, random_state=3)\n",
    "]\n",
    "\n",
    "# Print the split frequencies\n",
    "split_labels = [\"70%\", \"20%\", \"10%\"]\n",
    "for label, frequencies in zip(split_labels, split_frequencies):\n",
    "    print(\"\\n{} of patients ({} samples):\".format(label, frequencies.sum()))\n",
    "    print(frequencies)\n",
    "\n",
    "# Create separate lists of patients for each split\n",
    "split_70 = split_frequencies[0].index.tolist()\n",
    "split_20 = split_frequencies[1].index.tolist()\n",
    "split_10 = split_frequencies[2].index.tolist()\n",
    "\n",
    "# Print the separate lists\n",
    "print(\"\\nList of patients in the 70% split:\")\n",
    "print(split_70)\n",
    "\n",
    "print(\"\\nList of patients in the 20% split:\")\n",
    "print(split_20)\n",
    "\n",
    "print(\"\\nList of patients in the 10% split:\")\n",
    "print(split_10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New DataFrame for the 70% split:\n",
      "      Channel_I  Channel_aVF  Channel_V1  Channel_V6  Channel_CS 3-4  \\\n",
      "1             1            1           1           1               1   \n",
      "2             1            1           1           1               1   \n",
      "3             1            1           1           1               1   \n",
      "4             1            1           1           1               1   \n",
      "5             1            1           1           1               1   \n",
      "...         ...          ...         ...         ...             ...   \n",
      "2409          1            1           1           1               1   \n",
      "2410          1            1           1           1               1   \n",
      "2411          1            1           1           1               1   \n",
      "2413          1            1           1           1               1   \n",
      "2414          1            1           1           1               1   \n",
      "\n",
      "                                              File Path Patient Number  \\\n",
      "1     /rds/general/project/fsn-ai-ecg-data/live/afml...             60   \n",
      "2     /rds/general/project/fsn-ai-ecg-data/live/afml...             15   \n",
      "3     /rds/general/project/fsn-ai-ecg-data/live/afml...             81   \n",
      "4     /rds/general/project/fsn-ai-ecg-data/live/afml...             49   \n",
      "5     /rds/general/project/fsn-ai-ecg-data/live/afml...              8   \n",
      "...                                                 ...            ...   \n",
      "2409  /rds/general/project/fsn-ai-ecg-data/live/afml...          121.0   \n",
      "2410  /rds/general/project/fsn-ai-ecg-data/live/afml...          285.0   \n",
      "2411  /rds/general/project/fsn-ai-ecg-data/live/afml...          196.0   \n",
      "2413  /rds/general/project/fsn-ai-ecg-data/live/afml...          133.0   \n",
      "2414  /rds/general/project/fsn-ai-ecg-data/live/afml...          208.0   \n",
      "\n",
      "     Procedure  \n",
      "1          pre  \n",
      "2          pre  \n",
      "3          pre  \n",
      "4         post  \n",
      "5          pre  \n",
      "...        ...  \n",
      "2409      post  \n",
      "2410      post  \n",
      "2411      post  \n",
      "2413       pre  \n",
      "2414      post  \n",
      "\n",
      "[1438 rows x 8 columns]\n",
      "\n",
      "New DataFrame for the 20% split:\n",
      "      Channel_I  Channel_aVF  Channel_V1  Channel_V6  Channel_CS 3-4  \\\n",
      "5             1            1           1           1               1   \n",
      "11            1            1           1           1               1   \n",
      "14            1            1           1           1               1   \n",
      "24            1            1           1           1               1   \n",
      "29            1            1           1           1               1   \n",
      "...         ...          ...         ...         ...             ...   \n",
      "2398          1            1           1           1               1   \n",
      "2399          1            1           1           1               1   \n",
      "2400          1            1           1           1               1   \n",
      "2402          1            1           1           1               1   \n",
      "2413          1            1           1           1               1   \n",
      "\n",
      "                                              File Path Patient Number  \\\n",
      "5     /rds/general/project/fsn-ai-ecg-data/live/afml...              8   \n",
      "11    /rds/general/project/fsn-ai-ecg-data/live/afml...             57   \n",
      "14    /rds/general/project/fsn-ai-ecg-data/live/afml...             44   \n",
      "24    /rds/general/project/fsn-ai-ecg-data/live/afml...             44   \n",
      "29    /rds/general/project/fsn-ai-ecg-data/live/afml...             16   \n",
      "...                                                 ...            ...   \n",
      "2398  /rds/general/project/fsn-ai-ecg-data/live/afml...          135.0   \n",
      "2399  /rds/general/project/fsn-ai-ecg-data/live/afml...          145.0   \n",
      "2400  /rds/general/project/fsn-ai-ecg-data/live/afml...          129.0   \n",
      "2402  /rds/general/project/fsn-ai-ecg-data/live/afml...          262.0   \n",
      "2413  /rds/general/project/fsn-ai-ecg-data/live/afml...          133.0   \n",
      "\n",
      "     Procedure  \n",
      "5          pre  \n",
      "11         pre  \n",
      "14         pre  \n",
      "24        post  \n",
      "29         pre  \n",
      "...        ...  \n",
      "2398      post  \n",
      "2399       pre  \n",
      "2400       pre  \n",
      "2402      post  \n",
      "2413       pre  \n",
      "\n",
      "[428 rows x 8 columns]\n",
      "\n",
      "New DataFrame for the 10% split:\n",
      "      Channel_I  Channel_aVF  Channel_V1  Channel_V6  Channel_CS 3-4  \\\n",
      "17            1            1           1           1               1   \n",
      "28            1            1           1           1               1   \n",
      "36            1            1           1           1               1   \n",
      "51            1            1           1           1               1   \n",
      "83            1            1           1           1               1   \n",
      "...         ...          ...         ...         ...             ...   \n",
      "2369          1            1           1           1               1   \n",
      "2377          1            1           1           1               1   \n",
      "2378          1            1           1           1               1   \n",
      "2385          1            1           1           1               1   \n",
      "2393          1            1           1           1               1   \n",
      "\n",
      "                                              File Path Patient Number  \\\n",
      "17    /rds/general/project/fsn-ai-ecg-data/live/afml...             30   \n",
      "28    /rds/general/project/fsn-ai-ecg-data/live/afml...             10   \n",
      "36    /rds/general/project/fsn-ai-ecg-data/live/afml...              1   \n",
      "51    /rds/general/project/fsn-ai-ecg-data/live/afml...             80   \n",
      "83    /rds/general/project/fsn-ai-ecg-data/live/afml...             69   \n",
      "...                                                 ...            ...   \n",
      "2369  /rds/general/project/fsn-ai-ecg-data/live/afml...          236.0   \n",
      "2377  /rds/general/project/fsn-ai-ecg-data/live/afml...          122.0   \n",
      "2378  /rds/general/project/fsn-ai-ecg-data/live/afml...          275.0   \n",
      "2385  /rds/general/project/fsn-ai-ecg-data/live/afml...          298.0   \n",
      "2393  /rds/general/project/fsn-ai-ecg-data/live/afml...          177.0   \n",
      "\n",
      "     Procedure  \n",
      "17         pre  \n",
      "28         pre  \n",
      "36        post  \n",
      "51        post  \n",
      "83        post  \n",
      "...        ...  \n",
      "2369       pre  \n",
      "2377       pre  \n",
      "2378       pre  \n",
      "2385       pre  \n",
      "2393      post  \n",
      "\n",
      "[205 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create new DataFrames based on the patient numbers\n",
    "df_70 = final_df[final_df[\"Patient Number\"].isin(split_70)]\n",
    "df_20 = final_df[final_df[\"Patient Number\"].isin(split_20)]\n",
    "df_10 = final_df[final_df[\"Patient Number\"].isin(split_10)]\n",
    "\n",
    "# Print the new DataFrames\n",
    "print(\"New DataFrame for the 70% split:\")\n",
    "print(df_70)\n",
    "\n",
    "print(\"\\nNew DataFrame for the 20% split:\")\n",
    "print(df_20)\n",
    "\n",
    "print(\"\\nNew DataFrame for the 10% split:\")\n",
    "print(df_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of array_I_aVF_V1: (1438, 60000, 4)\n",
      "Shape of array_CS_3_4: (1438, 60000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create a list to store the arrays for each DataFrame\n",
    "arrays_I_aVF_V1 = []\n",
    "array_CS_3_4 = []\n",
    "\n",
    "# Iterate through each file path in each DataFrame\n",
    "for i, filepath in enumerate(df_70['File Path']):\n",
    "    txt_file = TxtFile(filepath)\n",
    "    \n",
    "    # Extract the data for channels I, aVF, V1, V6\n",
    "    channels_I_aVF_V1 = ['I', 'aVF', 'V1', 'V6']\n",
    "    data_I_aVF_V1 = txt_file.data[channels_I_aVF_V1].to_numpy()\n",
    "    \n",
    "    # Check the shape of the data array for consistency\n",
    "    if len(arrays_I_aVF_V1) > 0 and data_I_aVF_V1.shape != arrays_I_aVF_V1[0].shape:\n",
    "        raise ValueError(\"Inconsistent shape for channels I, aVF, V1, V6. Please check the data.\")\n",
    "    \n",
    "    arrays_I_aVF_V1.append(data_I_aVF_V1)\n",
    "    \n",
    "    # Extract the data for channel CS 3-4\n",
    "    channel_CS_3_4 = 'CS 3-4'\n",
    "    data_CS_3_4 = txt_file.data[channel_CS_3_4].to_numpy()\n",
    "    array_CS_3_4.append(data_CS_3_4)\n",
    "    \n",
    "\n",
    "# Stack the arrays together using np.stack\n",
    "array_I_aVF_V1 = np.stack(arrays_I_aVF_V1, axis=0)\n",
    "array_CS_3_4 = np.stack(array_CS_3_4, axis=0)\n",
    "\n",
    "# Print the shape of the final arrays\n",
    "print(\"Shape of array_I_aVF_V1:\", array_I_aVF_V1.shape)\n",
    "print(\"Shape of array_CS_3_4:\", array_CS_3_4.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making training dataset\n",
    "ECG_training = np.reshape(array_I_aVF_V1, (43140, 2000, 4))\n",
    "CS_training = np.reshape(array_CS_3_4,(43140,2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('ECG_training.npy', ECG_training)\n",
    "\n",
    "\n",
    "np.save('CS_training.npy', CS_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of array_I_aVF_V1: (428, 60000, 4)\n",
      "Shape of array_CS_3_4: (428, 60000)\n"
     ]
    }
   ],
   "source": [
    "# Create a list to store the arrays for each DataFrame\n",
    "arrays_I_aVF_V1 = []\n",
    "array_CS_3_4 = []\n",
    "\n",
    "# Iterate through each file path in each DataFrame\n",
    "for i, filepath in enumerate(df_20['File Path']):\n",
    "    txt_file = TxtFile(filepath)\n",
    "    \n",
    "    # Extract the data for channels I, aVF, V1, V6\n",
    "    channels_I_aVF_V1 = ['I', 'aVF', 'V1', 'V6']\n",
    "    data_I_aVF_V1 = txt_file.data[channels_I_aVF_V1].to_numpy()\n",
    "    \n",
    "    # Check the shape of the data array for consistency\n",
    "    if len(arrays_I_aVF_V1) > 0 and data_I_aVF_V1.shape != arrays_I_aVF_V1[0].shape:\n",
    "        raise ValueError(\"Inconsistent shape for channels I, aVF, V1, V6. Please check the data.\")\n",
    "    \n",
    "    arrays_I_aVF_V1.append(data_I_aVF_V1)\n",
    "    \n",
    "    # Extract the data for channel CS 3-4\n",
    "    channel_CS_3_4 = 'CS 3-4'\n",
    "    data_CS_3_4 = txt_file.data[channel_CS_3_4].to_numpy()\n",
    "    array_CS_3_4.append(data_CS_3_4)\n",
    "    \n",
    "\n",
    "# Stack the arrays together using np.stack\n",
    "array_I_aVF_V1 = np.stack(arrays_I_aVF_V1, axis=0)\n",
    "array_CS_3_4 = np.stack(array_CS_3_4, axis=0)\n",
    "\n",
    "# Print the shape of the final arrays\n",
    "print(\"Shape of array_I_aVF_V1:\", array_I_aVF_V1.shape)\n",
    "print(\"Shape of array_CS_3_4:\", array_CS_3_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making training dataset\n",
    "ECG_testing = np.reshape(array_I_aVF_V1, (12840, 2000, 4))\n",
    "CS_testing = np.reshape(array_CS_3_4,(12840,2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('ECG_testing.npy', ECG_testing)\n",
    "\n",
    "\n",
    "np.save('CS_testing.npy', CS_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of array_I_aVF_V1: (205, 60000, 4)\n",
      "Shape of array_CS_3_4: (205, 60000)\n"
     ]
    }
   ],
   "source": [
    "arrays_I_aVF_V1 = []\n",
    "array_CS_3_4 = []\n",
    "\n",
    "# Iterate through each file path in each DataFrame\n",
    "for i, filepath in enumerate(df_10['File Path']):\n",
    "    txt_file = TxtFile(filepath)\n",
    "    \n",
    "    # Extract the data for channels I, aVF, V1, V6\n",
    "    channels_I_aVF_V1 = ['I', 'aVF', 'V1', 'V6']\n",
    "    data_I_aVF_V1 = txt_file.data[channels_I_aVF_V1].to_numpy()\n",
    "    \n",
    "    # Check the shape of the data array for consistency\n",
    "    if len(arrays_I_aVF_V1) > 0 and data_I_aVF_V1.shape != arrays_I_aVF_V1[0].shape:\n",
    "        raise ValueError(\"Inconsistent shape for channels I, aVF, V1, V6. Please check the data.\")\n",
    "    \n",
    "    arrays_I_aVF_V1.append(data_I_aVF_V1)\n",
    "    \n",
    "    # Extract the data for channel CS 3-4\n",
    "    channel_CS_3_4 = 'CS 3-4'\n",
    "    data_CS_3_4 = txt_file.data[channel_CS_3_4].to_numpy()\n",
    "    array_CS_3_4.append(data_CS_3_4)\n",
    "    \n",
    "\n",
    "# Stack the arrays together using np.stack\n",
    "array_I_aVF_V1 = np.stack(arrays_I_aVF_V1, axis=0)\n",
    "array_CS_3_4 = np.stack(array_CS_3_4, axis=0)\n",
    "\n",
    "# Print the shape of the final arrays\n",
    "print(\"Shape of array_I_aVF_V1:\", array_I_aVF_V1.shape)\n",
    "print(\"Shape of array_CS_3_4:\", array_CS_3_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making training dataset\n",
    "ECG_validation = np.reshape(array_I_aVF_V1, (6150, 2000, 4))\n",
    "CS_validation = np.reshape(array_CS_3_4,(6150,2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('ECG_validation.npy', ECG_validation)\n",
    "\n",
    "np.save('CS_validation.npy', CS_validation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:r413]",
   "language": "python",
   "name": "conda-env-r413-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
